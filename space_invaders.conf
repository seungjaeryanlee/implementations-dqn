# Follows the hyperparameters in 'Extended Data Table 1' of the original paper

## Environment
ENV_NAME = SpaceInvaders-v4
# Paper specifies 50M "frames". Frames are skipped by 4.
ENV_STEPS = 12500000
FRAME_STACK = 4
FRAME_SKIP = 4
DISCOUNT = 0.99

## DQN - Experience Replay
BATCH_SIZE = 32
REPLAY_BUFFER_SIZE = 1000000
MIN_REPLAY_BUFFER_SIZE = 50000

## DQN - Target Network
TARGET_NET_UPDATE_FREQUENCY = 10000

## Epsilon Annealing
EPSILON_START = 1
EPSILON_END = 0.1
# Paper specifies 1M "frames". Frames are skipped by 4.
EPSILON_DURATION = 250000

## Update frequency
UPDATE_FREQUENCY = 4

## Optimizer - RMSProp
# NOTE(seungjaeryanlee): DeepMind DQN actually uses a very specific and sensitive RMSprop:
# https://github.com/deepmind/dqn/blob/9d9b1d13a2b491d6ebd4d046740c511c662bbe0f/dqn/NeuralQLearner.lua#L265
# Here are some reports from other people about this issue:
# - https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn.gin
# - https://github.com/ShangtongZhang/DeepRL/blob/master/examples.py
# - https://twitter.com/FlorinGogianu/status/1080139409658400769
RMSPROP_LR = 0.00025
RMSPROP_DECAY = 0.95
RMSPROP_EPSILON = 0.01
RMSPROP_MOMENTUM = 0
RMSPROP_WEIGHT_DECAY = 0
RMSPROP_IS_CENTERED = True

## Misc.
LOG_FREQUENCY = 100
EVAL_FREQUENCY = 10000
EVAL_EPISODES = 30
EVAL_EPSILON = 0.05
SAVE_DIR = saves/

# CPU
CPU_THREADS = 1
