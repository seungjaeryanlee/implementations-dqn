# Follows the hyperparameters in 'Extended Data Table 1' of the original paper

## Environment
# NOTE(seungjaeryanlee): Frameskip is done separately: check FRAME_SKIP
ENV_NAME = PongNoFrameskip-v4
ENV_FRAMES = 50000000
FRAME_STACK = 4
# NOTE(seungjaeryanlee): Use 3 for ENV_NAME=SpaceInvadersNoFrameskip-v4
FRAME_SKIP = 4
DISCOUNT = 0.99

## DQN - Experience Replay
BATCH_SIZE = 32
REPLAY_BUFFER_SIZE = 1000000
MIN_REPLAY_BUFFER_SIZE = 50000

## DQN - Target Network
TARGET_NET_UPDATE_FREQUENCY = 10000

## Epsilon Annealing
EPSILON_DECAY_START_VALUE = 1
EPSILON_DECAY_END_VALUE = 0.1
# Same as MIN_REPLAY_SIZE_BUFFER
EPSILON_DECAY_START_STEP = 50000
EPSILON_DECAY_END_STEP = 1000000

## Update frequency
UPDATE_FREQUENCY = 4

## Optimizer - RMSProp
# NOTE(seungjaeryanlee): DeepMind DQN actually uses a very specific and sensitive RMSprop:
# https://github.com/deepmind/dqn/blob/9d9b1d13a2b491d6ebd4d046740c511c662bbe0f/dqn/NeuralQLearner.lua#L265
# Here are some reports from other people about this issue:
# - https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn.gin
# - https://github.com/ShangtongZhang/DeepRL/blob/master/examples.py
# - https://twitter.com/FlorinGogianu/status/1080139409658400769
RMSPROP_LR = 0.00025
RMSPROP_DECAY = 0.95
RMSPROP_EPSILON = 0.01
RMSPROP_MOMENTUM = 0
RMSPROP_WEIGHT_DECAY = 0
RMSPROP_IS_CENTERED = True

## Misc.
LOG_FREQUENCY = 100
EVAL_FREQUENCY = 10000
EVAL_EPISODES = 30
EVAL_EPSILON = 0.05
SAVE_DIR = saves/

# CPU
CPU_THREADS = 1
